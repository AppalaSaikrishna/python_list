{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO8XN2O3vZaVnkjh893S7rD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AppalaSaikrishna/python_list/blob/main/10_02_2025.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZS5fh-lbRcQN",
        "outputId": "cff7290a-84b5-4dea-953e-f624a42134d8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyPDF2 in /usr/local/lib/python3.11/dist-packages (3.0.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install pyPDF2"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import PyPDF2\n",
        "from PyPDF2 import PdfFileReader"
      ],
      "metadata": {
        "id": "bzBedakHR27F"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "PyPDF2.__version__"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "fbVXs5RySGb9",
        "outputId": "5df68538-2b68-429f-8fd0-aed1891647f9"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'3.0.1'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pdf = open(\"/content/file1pdf.pdf\",\"rb\")\n",
        "pdf_reader = PyPDF2.PdfReader(pdf)\n",
        "print(\"Number of pages:\",len(pdf_reader.pages))\n",
        "page = pdf_reader.pages[1]\n",
        "print(page.extract_text())\n",
        "pdf.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FfIdxGltSRLA",
        "outputId": "fb78d5ba-4273-4107-d1be-72fefdcd3a92"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of pages: 35\n",
            " \n",
            " \n",
            " Development  Plan for Greater Mumbai 2014‐2034                                                                                                                                                                                                                                                      \n",
            "Acknowledgements  \n",
            "The Consultant  wishes to thank the following  individuals  from the Municipal  Corporation  of \n",
            "Greater Mumbai for their invaluable  support, insights and contributions  towards ‘Working  Paper 1 \n",
            "– Preparation  of Base Map’ for the preparation  of the Development  Plan for Greater Mumbai \n",
            "2014‐34. \n",
            " Mr. Subodh Kumar, IAS, Municipal  Commissioner;  \n",
            " Mr. Rajeev Kuknoor, Chief Engineer Development  Plan; \n",
            " Mr. Sudhir Ghate, Deputy Chief Engineer Development  Plan; \n",
            " Mr. A.G. Marathe, Deputy Chief Engineer Development  Plan; \n",
            " Mr. R. Balachandran,  Executive  Engineer and Town Planning Officer, Development  Plan. \n",
            " Our gratitude  to the following  experts for their invaluable  insights and support: \n",
            " \n",
            "Mr. V.K Phatak, Former Chief Town Planner (MMRDA);  \n",
            " Mr. A.N Kale, Former Chief Engineer, (DP); \n",
            " Mr. A. S Jain Former Dy. Chief Engineer, (DP). \n",
            " We wish to especially  thank MCGM officers, Mr. Jagdish Talreja, Mr. Dinesh Naik, Mr. Hiren \n",
            "Daftardar,  Ms. Anita Naik for their continual  support since the\n",
            " beginning  of the project and their \n",
            "help towards familiarization  and data collection.  They have been instrumental  in helping to \n",
            "contact various MCGM departments  as well as in helping to establish contact with personnel  from \n",
            "other government  departments  and organizations.  Many thanks for the MCGM team, for \n",
            "deploying  personnel,  particularly  Mr. Prasad Gharat, on extensive  field visits that have helped in \n",
            "understanding  actual ground conditions.  \n",
            " \n",
            "We apologize  if we have inadvertently  omitted anyone to whom acknowledgement  is due. We hope \n",
            "and anticipate  the work's usefulness  for the intended purpose. \n",
            " \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import PyPDF2, urllib , nltk\n",
        "from io import BytesIO\n",
        "from nltk import word_tokenize\n",
        "from nltk.corpus import stopwords"
      ],
      "metadata": {
        "id": "neSieQsrTjoV"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wFile = urllib.request.urlopen(\"https://www.udri.org/pdf/02%20working%20paper%201.pdf\")\n",
        "pdfreader = PyPDF2.PdfReader(BytesIO(wFile.read()))"
      ],
      "metadata": {
        "id": "LX2k3HY_VPq0"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TBpMqOANXlnU",
        "outputId": "d77671cf-1dd0-4e2a-8a4a-a58cf37c5aea"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WscKnuTQXqY0",
        "outputId": "016ac145-aae6-45dd-efa0-538e110e5a64"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from string import punctuation\n",
        "pageObj = pdfreader.pages[2]\n",
        "page2 = pageObj.extract_text()\n",
        "punctuations = ['(',')',';',':','[',']',',','...','.']\n",
        "tokens = word_tokenize(page2)\n",
        "stop_words = stopwords.words('english')\n",
        "keywords = [word for word in tokens if not word in stop_words and not word in punctuations]"
      ],
      "metadata": {
        "id": "1tq2irsNV9Lt"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "keywords"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i_VM4PQOZdD8",
        "outputId": "364523dc-8f1e-4823-8d04-723eb11a7c55"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Development',\n",
              " 'Plan',\n",
              " 'Greater',\n",
              " 'Mumbai',\n",
              " '2014‐2034',\n",
              " 'Table',\n",
              " 'Contents',\n",
              " 'The',\n",
              " 'Consultant',\n",
              " 'wishes',\n",
              " 'thank',\n",
              " 'following',\n",
              " 'individuals',\n",
              " 'Municipal',\n",
              " 'Corporation',\n",
              " 'Greater',\n",
              " 'Mumbai',\n",
              " 'invaluable',\n",
              " 'support',\n",
              " 'insights',\n",
              " 'contributions',\n",
              " 'towards',\n",
              " '‘',\n",
              " 'Working',\n",
              " 'Paper',\n",
              " '1',\n",
              " '–',\n",
              " 'Preparation',\n",
              " 'Base',\n",
              " 'Map',\n",
              " '’',\n",
              " 'preparation',\n",
              " 'Development',\n",
              " 'Plan',\n",
              " 'Greater',\n",
              " 'Mumbai',\n",
              " '2014‐34',\n",
              " '.............................................................................................................................',\n",
              " '..............',\n",
              " '3',\n",
              " 'Our',\n",
              " 'gratitude',\n",
              " 'following',\n",
              " 'experts',\n",
              " 'invaluable',\n",
              " 'insights',\n",
              " 'support',\n",
              " '............................',\n",
              " '3',\n",
              " 'We',\n",
              " 'wish',\n",
              " 'especially',\n",
              " 'thank',\n",
              " 'MCGM',\n",
              " 'officers',\n",
              " 'Mr.',\n",
              " 'Jagdish',\n",
              " 'Talreja',\n",
              " 'Mr.',\n",
              " 'Dinesh',\n",
              " 'Naik',\n",
              " 'Mr.',\n",
              " 'Hiren',\n",
              " 'Daftardar',\n",
              " 'Ms.',\n",
              " 'Anita',\n",
              " 'Naik',\n",
              " 'continual',\n",
              " 'support',\n",
              " 'since',\n",
              " 'beginning',\n",
              " 'project',\n",
              " 'help',\n",
              " 'towards',\n",
              " 'familiarization',\n",
              " 'data',\n",
              " 'collection',\n",
              " 'They',\n",
              " 'instrumental',\n",
              " 'helping',\n",
              " 'contact',\n",
              " 'various',\n",
              " 'MCGM',\n",
              " 'departments',\n",
              " 'well',\n",
              " 'helping',\n",
              " 'establish',\n",
              " 'contact',\n",
              " 'personnel',\n",
              " 'government',\n",
              " 'departments',\n",
              " 'organizations',\n",
              " 'Many',\n",
              " 'thanks',\n",
              " 'MCGM',\n",
              " 'team',\n",
              " 'deploying',\n",
              " 'personnel',\n",
              " 'particularly',\n",
              " 'Mr.',\n",
              " 'Prasad',\n",
              " 'Gharat',\n",
              " 'extensive',\n",
              " 'field',\n",
              " 'visits',\n",
              " 'helped',\n",
              " 'understanding',\n",
              " 'actual',\n",
              " 'ground',\n",
              " 'conditions',\n",
              " '........................................................................................',\n",
              " '3',\n",
              " 'BEST',\n",
              " '...............................................................................................................................',\n",
              " '.................',\n",
              " '5',\n",
              " 'Brihanmumbai',\n",
              " 'Electric',\n",
              " 'Supply',\n",
              " 'Transport',\n",
              " 'Undertaking',\n",
              " '..............................................................',\n",
              " '5',\n",
              " 'CIDCO',\n",
              " '...............................................................................................................................',\n",
              " '..............',\n",
              " '5',\n",
              " 'City',\n",
              " 'Industrial',\n",
              " 'Development',\n",
              " 'Corporation',\n",
              " '...............................................................................',\n",
              " '5',\n",
              " 'CTP',\n",
              " '...............................................................................................................................',\n",
              " '..................',\n",
              " '5',\n",
              " 'Comprehensive',\n",
              " 'Transportation',\n",
              " 'Plan',\n",
              " '...............................................................................................',\n",
              " '5',\n",
              " 'DP',\n",
              " '...............................................................................................................................',\n",
              " '....................',\n",
              " '5',\n",
              " 'Development',\n",
              " 'Plan',\n",
              " '..........................................................................................................................',\n",
              " '5',\n",
              " 'DPGM34',\n",
              " '...............................................................................................................................',\n",
              " '..........',\n",
              " '5',\n",
              " 'Development',\n",
              " 'Plan',\n",
              " 'Greater',\n",
              " 'Mumbai',\n",
              " '2034',\n",
              " '.......................................................................................',\n",
              " '5',\n",
              " 'DCR',\n",
              " '...............................................................................................................................',\n",
              " '..................',\n",
              " '5',\n",
              " 'Development',\n",
              " 'Control',\n",
              " 'Regulations',\n",
              " '...................................................................................................',\n",
              " '5',\n",
              " 'DGPS',\n",
              " '...........................................................................................................................',\n",
              " '....................',\n",
              " '5',\n",
              " 'Digital',\n",
              " 'Global',\n",
              " 'Positioning',\n",
              " 'System',\n",
              " '...................................................................................................',\n",
              " '5',\n",
              " 'DPGM',\n",
              " '...............................................................................................................................',\n",
              " '..............',\n",
              " '5',\n",
              " 'Development',\n",
              " 'Plan',\n",
              " 'Greater',\n",
              " 'Mumbai',\n",
              " '...........................................................................................',\n",
              " '5',\n",
              " 'ELU',\n",
              " '...............................................................................................................................',\n",
              " '..................',\n",
              " '5',\n",
              " 'Existing',\n",
              " 'Land',\n",
              " 'use',\n",
              " '.............................................................................................................................',\n",
              " '5',\n",
              " 'FSI',\n",
              " '...............................................................................................................................',\n",
              " '....................',\n",
              " '5',\n",
              " 'Floor',\n",
              " 'Space',\n",
              " 'Index',\n",
              " '............................................................................................................................',\n",
              " '5',\n",
              " 'GIS',\n",
              " '...............................................................................................................................',\n",
              " '...................',\n",
              " '5']"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "name_list = list()\n",
        "check = ['Mr.', 'Mrs.', 'Ms.']\n",
        "for idx, token in enumerate(tokens):\n",
        "  if token.startswith(tuple(check)) and idx < len(tokens)-1:\n",
        "    name = token + tokens[idx+1] + ' ' + tokens[idx+2]\n",
        "    name_list.append(name)\n",
        "\n",
        "print(name_list)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b3Sd-1OyX17O",
        "outputId": "ae3c4f8d-fc8b-4d0f-f3ab-7613e63a8dc8"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Mr.Jagdish Talreja', 'Mr.Dinesh Naik', 'Mr.Hiren Daftardar', 'Ms.Anita Naik', 'Mr.Prasad Gharat']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wFile.close()"
      ],
      "metadata": {
        "id": "MyvfzvRpZPp-"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install python-docx"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GxXsVsTfZpVy",
        "outputId": "655834cc-0948-4fd6-9ac4-ecc217a64f1f"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting python-docx\n",
            "  Downloading python_docx-1.1.2-py3-none-any.whl.metadata (2.0 kB)\n",
            "Requirement already satisfied: lxml>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from python-docx) (5.3.0)\n",
            "Requirement already satisfied: typing-extensions>=4.9.0 in /usr/local/lib/python3.11/dist-packages (from python-docx) (4.12.2)\n",
            "Downloading python_docx-1.1.2-py3-none-any.whl (244 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/244.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━\u001b[0m \u001b[32m153.6/244.3 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m244.3/244.3 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: python-docx\n",
            "Successfully installed python-docx-1.1.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import docx"
      ],
      "metadata": {
        "id": "EWU3djRYabOU"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "doc = open(\"/content/PROBLEM STATEMENT.docx\",\"rb\")\n",
        "document = docx.Document(doc)"
      ],
      "metadata": {
        "id": "izqMhEtjadNy"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "docu=\"\"\n",
        "for para in document.paragraphs:\n",
        "  docu += para.text\n",
        "print(docu)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WRQa_z8_atEQ",
        "outputId": "5fd02dc9-9642-4cd4-b469-627b712ca9b9"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PROBLEM STATEMENT In many fields, we deal with large amounts of text that contain important details like names of people, organizations, locations, dates, and other key information. Going through these texts manually to find and highlight these details can take a lot of time and effort, and it’s easy to miss something important. On top of that, some names are made up of multiple words (like \"New York City\" or \"Barack Obama\"), which makes it even harder to identify them correctly.This project aims to create a simple, user-friendly tool that automatically finds and highlights these important details in text. Users can upload their text files, and the app will quickly process them to identify and categorize named entities, even if they consist of several words. The goal is to save time and make it easier for people—whether they’re researchers, students, or educators—to extract valuable information from documents without needing to know anything about machine learning or complex tech.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(document.paragraphs)):\n",
        "  print(\"The content of the paragraph \"+str(i)+\" is : \"+ document.paragraphs[i].text+\"\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gohyq0PYcOA0",
        "outputId": "ea329d3b-606e-4ca0-cfa5-88372d37cc24"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The content of the paragraph 0 is : PROBLEM STATEMENT \n",
            "\n",
            "The content of the paragraph 1 is : In many fields, we deal with large amounts of text that contain important details like names of people, organizations, locations, dates, and other key information. Going through these texts manually to find and highlight these details can take a lot of time and effort, and it’s easy to miss something important. On top of that, some names are made up of multiple words (like \"New York City\" or \"Barack Obama\"), which makes it even harder to identify them correctly.\n",
            "\n",
            "The content of the paragraph 2 is : \n",
            "\n",
            "The content of the paragraph 3 is : This project aims to create a simple, user-friendly tool that automatically finds and highlights these important details in text. Users can upload their text files, and the app will quickly process them to identify and categorize named entities, even if they consist of several words. The goal is to save time and make it easier for people—whether they’re researchers, students, or educators—to extract valuable information from documents without needing to know anything about machine learning or complex tech.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install bs4\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NrG4t0fhczKR",
        "outputId": "cf2ed3fb-3324-4475-f9e0-2f3909395e8f"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting bs4\n",
            "  Downloading bs4-0.0.2-py2.py3-none-any.whl.metadata (411 bytes)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (from bs4) (4.13.3)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->bs4) (2.6)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->bs4) (4.12.2)\n",
            "Downloading bs4-0.0.2-py2.py3-none-any.whl (1.2 kB)\n",
            "Installing collected packages: bs4\n",
            "Successfully installed bs4-0.0.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import urllib.request as urllib2\n",
        "from bs4 import BeautifulSoup"
      ],
      "metadata": {
        "id": "T4TMxAd8c_Oj"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = urllib2.urlopen('https://en.wikipedia.org/wiki/Machine_learning')\n",
        "html_doc = response.read()"
      ],
      "metadata": {
        "id": "Zl__CP5edMmw"
      },
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "soup = BeautifulSoup(html_doc, 'html.parser')\n",
        "strhtml = soup.prettify()\n",
        "print (strhtml[:5000])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r3TD5shrdWWQ",
        "outputId": "49c906c6-0486-41c8-ea5c-477c385e1d65"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<!DOCTYPE html>\n",
            "<html class=\"client-nojs vector-feature-language-in-header-enabled vector-feature-language-in-main-page-header-disabled vector-feature-page-tools-pinned-disabled vector-feature-toc-pinned-clientpref-1 vector-feature-main-menu-pinned-disabled vector-feature-limited-width-clientpref-1 vector-feature-limited-width-content-enabled vector-feature-custom-font-size-clientpref-1 vector-feature-appearance-pinned-clientpref-1 vector-feature-night-mode-enabled skin-theme-clientpref-day vector-sticky-header-enabled vector-toc-available\" dir=\"ltr\" lang=\"en\">\n",
            " <head>\n",
            "  <meta charset=\"utf-8\"/>\n",
            "  <title>\n",
            "   Machine learning - Wikipedia\n",
            "  </title>\n",
            "  <script>\n",
            "   (function(){var className=\"client-js vector-feature-language-in-header-enabled vector-feature-language-in-main-page-header-disabled vector-feature-page-tools-pinned-disabled vector-feature-toc-pinned-clientpref-1 vector-feature-main-menu-pinned-disabled vector-feature-limited-width-clientpref-1 vector-feature-limited-width-content-enabled vector-feature-custom-font-size-clientpref-1 vector-feature-appearance-pinned-clientpref-1 vector-feature-night-mode-enabled skin-theme-clientpref-day vector-sticky-header-enabled vector-toc-available\";var cookie=document.cookie.match(/(?:^|; )enwikimwclientpreferences=([^;]+)/);if(cookie){cookie[1].split('%2C').forEach(function(pref){className=className.replace(new RegExp('(^| )'+pref.replace(/-clientpref-\\w+$|[^\\w-]+/g,'')+'-clientpref-\\\\w+( |$)'),'$1'+pref+'$2');});}document.documentElement.className=className;}());RLCONF={\"wgBreakFrames\":false,\"wgSeparatorTransformTable\":[\"\",\"\"],\"wgDigitTransformTable\":[\"\",\"\"],\"wgDefaultDateFormat\":\"dmy\",\n",
            "\"wgMonthNames\":[\"\",\"January\",\"February\",\"March\",\"April\",\"May\",\"June\",\"July\",\"August\",\"September\",\"October\",\"November\",\"December\"],\"wgRequestId\":\"2d41b2d2-19f6-433f-9fe6-7577c077ae5f\",\"wgCanonicalNamespace\":\"\",\"wgCanonicalSpecialPageName\":false,\"wgNamespaceNumber\":0,\"wgPageName\":\"Machine_learning\",\"wgTitle\":\"Machine learning\",\"wgCurRevisionId\":1273675406,\"wgRevisionId\":1273675406,\"wgArticleId\":233488,\"wgIsArticle\":true,\"wgIsRedirect\":false,\"wgAction\":\"view\",\"wgUserName\":null,\"wgUserGroups\":[\"*\"],\"wgCategories\":[\"Webarchive template wayback links\",\"Articles with short description\",\"Short description is different from Wikidata\",\"Articles with excerpts\",\"Wikipedia articles needing clarification from January 2024\",\"Commons category link from Wikidata\",\"Machine learning\",\"Cybernetics\",\"Learning\",\"Definition\"],\"wgPageViewLanguage\":\"en\",\"wgPageContentLanguage\":\"en\",\"wgPageContentModel\":\"wikitext\",\"wgRelevantPageName\":\"Machine_learning\",\"wgRelevantArticleId\":233488,\"wgIsProbablyEditable\":true,\n",
            "\"wgRelevantPageIsProbablyEditable\":true,\"wgRestrictionEdit\":[],\"wgRestrictionMove\":[],\"wgNoticeProject\":\"wikipedia\",\"wgCiteReferencePreviewsActive\":false,\"wgFlaggedRevsParams\":{\"tags\":{\"status\":{\"levels\":1}}},\"wgMediaViewerOnClick\":true,\"wgMediaViewerEnabledByDefault\":true,\"wgPopupsFlags\":0,\"wgVisualEditor\":{\"pageLanguageCode\":\"en\",\"pageLanguageDir\":\"ltr\",\"pageVariantFallbacks\":\"en\"},\"wgMFDisplayWikibaseDescriptions\":{\"search\":true,\"watchlist\":true,\"tagline\":false,\"nearby\":true},\"wgWMESchemaEditAttemptStepOversample\":false,\"wgWMEPageLength\":100000,\"wgEditSubmitButtonLabelPublish\":true,\"wgULSPosition\":\"interlanguage\",\"wgULSisCompactLinksEnabled\":false,\"wgVector2022LanguageInHeader\":true,\"wgULSisLanguageSelectorEmpty\":false,\"wgWikibaseItemId\":\"Q2539\",\"wgCheckUserClientHintsHeadersJsApi\":[\"brands\",\"architecture\",\"bitness\",\"fullVersionList\",\"mobile\",\"model\",\"platform\",\"platformVersion\"],\"GEHomepageSuggestedEditsEnableTopics\":true,\"wgGETopicsMatchModeEnabled\":false,\n",
            "\"wgGEStructuredTaskRejectionReasonTextInputEnabled\":false,\"wgGELevelingUpEnabledForUser\":false};RLSTATE={\"ext.globalCssJs.user.styles\":\"ready\",\"site.styles\":\"ready\",\"user.styles\":\"ready\",\"ext.globalCssJs.user\":\"ready\",\"user\":\"ready\",\"user.options\":\"loading\",\"ext.cite.styles\":\"ready\",\"ext.math.styles\":\"ready\",\"skins.vector.search.codex.styles\":\"ready\",\"skins.vector.styles\":\"ready\",\"skins.vector.icons\":\"ready\",\"jquery.makeCollapsible.styles\":\"ready\",\"ext.wikimediamessages.styles\":\"ready\",\"ext.visualEditor.desktopArticleTarget.noscript\":\"ready\",\"ext.uls.interlanguage\":\"ready\",\"wikibase.client.init\":\"ready\",\"ext.wikimediaBadges\":\"ready\"};RLPAGEMODULES=[\"ext.cite.ux-enhancements\",\"mediawiki.page.media\",\"ext.scribunto.logs\",\"site\",\"mediawiki.page.ready\",\"jquery.makeCollapsible\",\"mediawiki.toc\",\"skins.vector.js\",\"ext.centralNotice.geoIP\",\"ext.centralNotice.startUp\",\"ext.gadget.ReferenceTooltips\",\"ext.gadget.switcher\",\"ext.urlShortener.toolbar\",\"ext.centralauth.centralautologin\",\n",
            "\"mmv.bootstrap\",\"ext.popups\",\"ext.visualEditor.desktopArticleTarget.init\",\"ext.visualEditor.targetLoader\",\"ext.echo.centralauth\",\"ext.eventLogging\",\"ext.wikimediaEvents\",\"ext.navigationTiming\",\"ext.uls.interface\",\"ext.cx.eventlogging.campaigns\",\"ext.cx.uls.quick.actions\",\"wikibase.client.vector-2022\",\"ext.checkUser.clientHints\",\"ext.growthExperiments.SuggestedEditSession\"];\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gbEvxPh5dtIe"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}